{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XRxHiKdGHiT"
      },
      "source": [
        "## Image classification with deep learning methods.\n",
        "\n",
        "-- Description --\n",
        "\n",
        "When you train the network, it is recommended to use the GPU resources of your computer.\n",
        "This will help you to learn the \"know how\" of setting up a working Python environment on your computer.\n",
        "In the case of unavailable Nvidia hardware or problems with your Python environment you can use Google Colab.\n",
        "Please go to the menu, Runtime - Change runtime type, and select **GPU** as the hardware accelerator.\n",
        "Although you used your computer successfuly it is highly recommended to give a try to Google Colab environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eq1KWmR3HWYV",
        "outputId": "70335e1d-8389-40dd-b66e-80604e07bdf6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: medmnist in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from medmnist) (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from medmnist) (2.0.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from medmnist) (1.2.2)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from medmnist) (0.19.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from medmnist) (4.66.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from medmnist) (9.4.0)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.10/dist-packages (from medmnist) (0.6.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from medmnist) (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from medmnist) (0.17.1+cu121)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire->medmnist) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->medmnist) (2.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->medmnist) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->medmnist) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->medmnist) (2024.1)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (1.11.4)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (3.3)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (2024.2.12)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (1.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (24.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->medmnist) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->medmnist) (3.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (1.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->medmnist) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->medmnist) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->medmnist) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "# These libraries should be sufficient for this Practice.\n",
        "# However, if any other library is needed, please install it by yourself.\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torch.utils.data as data\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import colors\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "!pip install medmnist\n",
        "import medmnist\n",
        "from medmnist import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvH6SWlDQBMf",
        "outputId": "01ee5848-23d0-4f39-d963-046c242482d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using downloaded and verified file: /root/.medmnist/bloodmnist.npz\n",
            "Using downloaded and verified file: /root/.medmnist/bloodmnist.npz\n",
            "Using downloaded and verified file: /root/.medmnist/bloodmnist.npz\n"
          ]
        }
      ],
      "source": [
        "# Parameters\n",
        "NUM_EPOCHS = 20\n",
        "BATCH_SIZE = 32\n",
        "lr = 0.0001\n",
        "DOWNLOAD_OK = True\n",
        "data_flag = 'bloodmnist'\n",
        "im_size = 224\n",
        "info = INFO[data_flag]\n",
        "task = info['task']\n",
        "n_channels = info['n_channels']\n",
        "num_classes = len(info['label'])\n",
        "N_IMAGES = 1000\n",
        "data_labels = info['label']\n",
        "\n",
        "# Tupla que contiene los valores asociados a los parámetros mostrados anteriormente,\n",
        "# con el fin de mejorar la organización de dichos parámetros.\n",
        "parameters = {\"num_epochs\": NUM_EPOCHS, \"batch_size\": BATCH_SIZE, \"lr\": lr, \"download_ok\": DOWNLOAD_OK,\n",
        "              \"data_flag\": data_flag, \"im_size\": im_size,\"info_task\": task, \"n_channels\": n_channels,\n",
        "              \"num_classes\": num_classes,\"n_images\":N_IMAGES, \"data_labels\": data_labels}\n",
        "\n",
        "# Preprocesado de datos mediante la definición de la transformación de datos\n",
        "def preprocessing_data(parameters):\n",
        "  data_transform = transforms.Compose([\n",
        "    transforms.Resize((parameters[\"im_size\"],parameters[\"im_size\"])),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[.5], std=[.5])\n",
        "  ])\n",
        "\n",
        "  full_train_dataset = BloodMNIST(split=\"train\", transform=data_transform, download=True)\n",
        "  full_valid_dataset = BloodMNIST(split=\"val\", transform=data_transform, download=True)\n",
        "  full_test_dataset = BloodMNIST(split=\"test\", transform=data_transform, download=True)\n",
        "\n",
        "  idx_train = np.random.choice(len(full_train_dataset),size=1000,replace=False)\n",
        "  train_dataset = [full_train_dataset[i] for i in idx_train]\n",
        "\n",
        "  idx_valid = np.random.choice(len(full_valid_dataset),size=300,replace=False)\n",
        "  valid_dataset = [full_valid_dataset[i] for i in idx_valid]\n",
        "\n",
        "  idx_test = np.random.choice(len(full_test_dataset),size=600,replace=False)\n",
        "  test_dataset = [full_test_dataset[i] for i in idx_test]\n",
        "\n",
        "  train_loader = data.DataLoader(dataset=train_dataset, batch_size=parameters[\"batch_size\"], shuffle=True)\n",
        "  valid_loader = data.DataLoader(dataset=valid_dataset, batch_size=parameters[\"batch_size\"], shuffle=False)\n",
        "  test_loader = data.DataLoader(dataset=test_dataset, batch_size=parameters[\"batch_size\"], shuffle=True)\n",
        "\n",
        "  return train_loader, valid_loader, test_loader\n",
        "\n",
        "train_loader,valid_loader,test_loader = preprocessing_data(parameters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMVmgmpHGrtx"
      },
      "outputs": [],
      "source": [
        "## Your code\n",
        "\n",
        "# Función que permite visualizar los aspectos fundamentales sobre cada\n",
        "# dataset que se haya descargado/cargado previamente.\n",
        "def dataset_visualizer(dataset,length_montage):\n",
        "  for i in range(0,length_montage*length_montage):\n",
        "    img = dataset[i][0]\n",
        "    label = str(dataset[i][1]).replace('[','').replace(']','')\n",
        "    figure = plt.figure(figsize=(2,2))\n",
        "    plt.imshow(img.permute(1,2,0))\n",
        "    plt.title(data_labels[label])\n",
        "    plt.axis(\"off\")\n",
        "  plt.show()\n",
        "\n",
        "# Visualizador de las imágenes a través de un pipeline DataLoader\n",
        "def dataloader_visualizer(data_loader,num_batches):\n",
        "  for batch_idx, (features, labels) in enumerate(data_loader):\n",
        "      if batch_idx >= num_batches:\n",
        "        break\n",
        "      for i in range(len(features)):\n",
        "        img = features[i].squeeze()\n",
        "        label = str(labels[i]).replace('tensor([','').replace('])','')\n",
        "        plt.figure(figsize=(2,2))\n",
        "        plt.title(label)\n",
        "        plt.imshow(img.permute(1,2,0))\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "def net_model_visualizer(net_model):\n",
        "  print(net_model)\n",
        "  print('Total Parameters:',\n",
        "       sum([torch.numel(p) for p in net_model.parameters()])\n",
        "  )\n",
        "  print('Trainable Parameters:',\n",
        "       sum([torch.numel(p) for p in net_model.parameters() if p.requires_grad])\n",
        "  )\n",
        "\n",
        "#dataloader_visualizer(train_loader,1)\n",
        "#dataloader_visualizer(valid_loader,1)\n",
        "#dataloader_visualizer(test_loader,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8OH_2iLU6oZ"
      },
      "outputs": [],
      "source": [
        "# Define a simple CNN model\n",
        "\n",
        "def convolution(in_channels,out_channels):\n",
        "  layer = nn.Sequential(\n",
        "      nn.Conv2d(in_channels,out_channels,3),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(2,2)\n",
        "  )\n",
        "\n",
        "  return layer\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, parameters):\n",
        "        super(Net, self).__init__()\n",
        "        #Define the desired deep learning model\n",
        "        #Your code\n",
        "\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(parameters[\"n_channels\"],32,3),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2,2)\n",
        "        )\n",
        "\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(32,64,3),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2,2)\n",
        "        )\n",
        "\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(64,128,3),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2,2)\n",
        "        )\n",
        "\n",
        "        self.layer4 = nn.Sequential(\n",
        "            nn.Conv2d(128,256,3),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2,2)\n",
        "        )\n",
        "\n",
        "        self.fc1 = nn.Linear(256,128)\n",
        "        self.drop1 = nn.Dropout(0.33)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128,parameters[\"num_classes\"])\n",
        "\n",
        "        #End your code\n",
        "\n",
        "    def forward(self, x):\n",
        "        #Your code\n",
        "        #print(1, x.shape)\n",
        "        x = self.layer1(x)\n",
        "        #print(2, x.shape)\n",
        "        x = self.layer2(x)\n",
        "        #print(3, x.shape)\n",
        "        x = self.layer3(x)\n",
        "        #print(4, x.shape)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        #print(5, x.shape)\n",
        "        x = F.adaptive_avg_pool2d(x,1).reshape(x.size(0),-1)\n",
        "\n",
        "        #print(6, x.shape)\n",
        "        x = self.fc1(x)\n",
        "        x = self.drop1(x)\n",
        "        x = self.relu1(x)\n",
        "        #print(7, x.shape)\n",
        "        x = self.fc2(x)\n",
        "        #print(8, x.shape)\n",
        "\n",
        "        return x\n",
        "\n",
        "#def double_convultion(in_channels,out_channels):\n",
        "\n",
        "#  conv_op = nn.Sequential(\n",
        "#      nn.Conv2d(in_channels,out_channels,1),\n",
        "#      nn.ReLU(True),\n",
        "#      nn.Conv2d(out_channels,out_channels,1),\n",
        "#      nn.ReLU(True)\n",
        "#  )\n",
        "\n",
        "#  return conv_op\n",
        "\n",
        "#class UNetCNN(nn.Module):\n",
        "#  def __init__(self,parameters):\n",
        "#    super(UNetCNN,self).__init__()\n",
        "\n",
        "#    self.max_pool2d = nn.MaxPool2d(2,2)\n",
        "\n",
        "#    self.down_conv1 = double_convultion(parameters[\"n_channels\"],64)\n",
        "#    self.down_conv2 = double_convultion(64,128)\n",
        "#    self.down_conv3 = double_convultion(128,256)\n",
        "#    self.down_conv4 = double_convultion(256,512)\n",
        "#    self.down_conv5 = double_convultion(512,1024)\n",
        "\n",
        "#    self.up_trans1 = nn.ConvTranspose2d(1024,512,2,2)\n",
        "#    self.up_conv1 = double_convultion(1024,512)\n",
        "#    self.up_trans2 = nn.ConvTranspose2d(512,256,2,2)\n",
        "#    self.up_conv2 = double_convultion(512,256)\n",
        "#    self.up_trans3 = nn.ConvTranspose2d(256,128,2,2)\n",
        "#    self.up_conv3 = double_convultion(256,128)\n",
        "#    self.up_trans4 = nn.ConvTranspose2d(128,64,2,2)\n",
        "#    self.up_conv4 = double_convultion(128,64)\n",
        "\n",
        "#    self.final_layer = nn.Conv2d(64,parameters[\"num_classes\"],1)\n",
        "\n",
        "#  def forward(self,x):\n",
        "#    print(x.shape)\n",
        "#    down1 = self.down_conv1(x)\n",
        "#    print(down1.shape)\n",
        "#    down2 = self.max_pool2d(down1)\n",
        "#    print(down2.shape)\n",
        "#    down3 = self.down_conv2(down2)\n",
        "#    print(down3.shape)\n",
        "#    down4 = self.max_pool2d(down3)\n",
        "#    print(down4.shape)\n",
        "#    down5 = self.down_conv3(down4)\n",
        "#    print(down5.shape)\n",
        "#    down6 = self.max_pool2d(down5)\n",
        "#    print(down6.shape)\n",
        "#    down7 = self.down_conv4(down6)\n",
        "#    print(down7.shape)\n",
        "#    down8 = self.max_pool2d(down7)\n",
        "#    print(down8.shape)\n",
        "#    down9 = self.down_conv5(down8)\n",
        "#    print(down9.shape)\n",
        "\n",
        "#    up1 = self.up_trans1(down9)\n",
        "#    print(up1.shape)\n",
        "#    x = self.up_conv1(torch.cat([down7,up1],1))\n",
        "\n",
        "#    up2 = self.up_trans2(x)\n",
        "#    print(up2.shape)\n",
        "#    x = self.up_conv2(torch.cat([down5,up2],1))\n",
        "\n",
        "#    up3 = self.up_trans3(x)\n",
        "#    print(up3.shape)\n",
        "#    x = self.up_conv3(torch.cat([down3,up3],1))\n",
        "\n",
        "#    up4 = self.up_trans4(x)\n",
        "#    print(up4.shape)\n",
        "#    x = self.up_conv4(torch.cat([down1,up4],1))\n",
        "\n",
        "#    final_layer = self.final_layer(x)\n",
        "#    return final_layer\n",
        "\n",
        "#class ResNet18(nn.Module):\n",
        "#  def __init__(self,parameters):\n",
        "#    super(ResNet18, self).__init__()\n",
        "\n",
        "#    self.conv1 = nn.Conv2d(parameters[\"n_channels\"],64,7,2,3)\n",
        "#    self.bn1 = nn.BatchNorm2d(64)\n",
        "#    self.maxpool = nn.MaxPool2d(3,2,1)\n",
        "\n",
        "#    self.conv2_block1 = nn.Sequential(\n",
        "#        nn.Conv2d(64,64,3,1,1),\n",
        "#        nn.BatchNorm2d(64),\n",
        "#        nn.ReLU(True),\n",
        "#        nn.Conv2d(64,64,3,1,1),\n",
        "#        nn.BatchNorm2d(64)\n",
        "#    )\n",
        "\n",
        "#    self.conv2_block2 = nn.Sequential(\n",
        "#        nn.Conv2d(64,64,3,1,1),\n",
        "#        nn.BatchNorm2d(64),\n",
        "#        nn.ReLU(True),\n",
        "#        nn.Conv2d(64,64,3,1,1),\n",
        "#        nn.BatchNorm2d(64)\n",
        "#    )\n",
        "\n",
        "#    self.conv3_block1 = nn.Sequential(\n",
        "#        nn.Conv2d(64,128,3,2,1),\n",
        "#        nn.BatchNorm2d(128),\n",
        "#        nn.ReLU(True),\n",
        "#        nn.Conv2d(128,128,3,1,1),\n",
        "#        nn.BatchNorm2d(128)\n",
        "#    )\n",
        "\n",
        "#    self.conv3_block2 = nn.Sequential(\n",
        "#        nn.Conv2d(128,128,3,1,1),\n",
        "#        nn.BatchNorm2d(128),\n",
        "#        nn.ReLU(True),\n",
        "#        nn.Conv2d(128,128,3,1,1),\n",
        "#        nn.BatchNorm2d(128)\n",
        "#    )\n",
        "\n",
        "#    self.conv4_block1 = nn.Sequential(\n",
        "#        nn.Conv2d(128,256,3,2,1),\n",
        "#        nn.BatchNorm2d(256),\n",
        "#        nn.ReLU(True),\n",
        "#        nn.Conv2d(256,256,3,1,1),\n",
        "#        nn.BatchNorm2d(256)\n",
        "#    )\n",
        "\n",
        "#    self.conv4_block2 = nn.Sequential(\n",
        "#        nn.Conv2d(256,256,3,1,1),\n",
        "#        nn.BatchNorm2d(256),\n",
        "#        nn.ReLU(True),\n",
        "#        nn.Conv2d(256,256,3,1,1),\n",
        "#        nn.BatchNorm2d(256)\n",
        "#    )\n",
        "\n",
        "#    self.conv5_block1 = nn.Sequential(\n",
        "#        nn.Conv2d(256,512,3,2,1),\n",
        "#        nn.BatchNorm2d(512),\n",
        "#        nn.ReLU(True),\n",
        "#        nn.Conv2d(512,512,3,1,1),\n",
        "#        nn.BatchNorm2d(512)\n",
        "#    )\n",
        "\n",
        "#    self.conv5_block2 = nn.Sequential(\n",
        "#        nn.Conv2d(512,512,3,1,1),\n",
        "#        nn.BatchNorm2d(512),\n",
        "#        nn.ReLU(True),\n",
        "#        nn.Conv2d(512,512,3,1,1),\n",
        "#        nn.BatchNorm2d(512)\n",
        "#    )\n",
        "\n",
        "#    self.avgpool = nn.AdaptiveAvgPool2d(output_size=(1,1))\n",
        "#    self.fc = nn.Linear(512,1000)\n",
        "\n",
        "#  def forward(self,x):\n",
        "#    print(x.shape)\n",
        "#    x = self.conv1(x)\n",
        "#    x = self.bn1(x)\n",
        "#    x = self.maxpool(x)\n",
        "\n",
        "#    print(x.shape)\n",
        "#    x = self.conv2_block1(x)\n",
        "#    x = self.conv2_block2(x)\n",
        "#    x = self.conv3_block1(x)\n",
        "#    x = self.conv3_block2(x)\n",
        "#    x = self.conv4_block1(x)\n",
        "#    x = self.conv4_block2(x)\n",
        "#    x = self.conv5_block1(x)\n",
        "#    x = self.conv5_block2(x)\n",
        "\n",
        "#    x = self.avgpool(x)\n",
        "#    x = self.fc(x)\n",
        "\n",
        "#    return x\n",
        "\n",
        "        #End your code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENrFXnaOVt0i",
        "outputId": "08aff219-9baf-49e4-d73d-9b01e8e35c85"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 63/63 [02:02<00:00,  1.94s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 0, 'train_acc': 0.0, 'train_loss': 2.048327254870581}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 19/19 [00:15<00:00,  1.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 0, 'valid_acc': 0.0, 'batch_loss': 1.9437567623038041}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 63/63 [02:05<00:00,  1.99s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 1, 'train_acc': 0.0, 'train_loss': 1.9408187695911951}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 19/19 [00:14<00:00,  1.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 1, 'valid_acc': 0.0, 'batch_loss': 1.7419948389655666}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 63/63 [01:57<00:00,  1.87s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 2, 'train_acc': 0.0, 'train_loss': 1.7128869124821253}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 19/19 [00:15<00:00,  1.25it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 2, 'valid_acc': 0.0, 'batch_loss': 1.4017423391342163}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 63/63 [01:55<00:00,  1.84s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 3, 'train_acc': 0.0, 'train_loss': 1.5819395799485465}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 19/19 [00:14<00:00,  1.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 3, 'valid_acc': 0.0, 'batch_loss': 1.3404574519709538}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 63/63 [01:54<00:00,  1.82s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 4, 'train_acc': 0.0, 'train_loss': 1.5206478493554252}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 19/19 [00:14<00:00,  1.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 4, 'valid_acc': 0.0, 'batch_loss': 1.2766818812018947}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 63/63 [01:55<00:00,  1.84s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 5, 'train_acc': 0.0, 'train_loss': 1.435350892089662}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 19/19 [00:15<00:00,  1.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 5, 'valid_acc': 0.0, 'batch_loss': 1.1965022275322361}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 63/63 [02:05<00:00,  1.99s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 6, 'train_acc': 0.0, 'train_loss': 1.415076684384119}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 19/19 [00:14<00:00,  1.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 6, 'valid_acc': 0.0, 'batch_loss': 1.1733337734874927}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 63/63 [01:59<00:00,  1.90s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 7, 'train_acc': 0.0, 'train_loss': 1.3624849697900197}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 19/19 [00:14<00:00,  1.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 7, 'valid_acc': 0.0, 'batch_loss': 1.1323816995871694}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 63/63 [01:57<00:00,  1.86s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 8, 'train_acc': 0.0, 'train_loss': 1.304052147600386}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 19/19 [00:15<00:00,  1.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 8, 'valid_acc': 0.0, 'batch_loss': 1.074097925110867}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 63/63 [01:55<00:00,  1.83s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 9, 'train_acc': 0.0, 'train_loss': 1.2430341480270264}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 19/19 [00:14<00:00,  1.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 9, 'valid_acc': 0.0, 'batch_loss': 1.0038420871684426}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 63/63 [02:00<00:00,  1.92s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 10, 'train_acc': 0.0, 'train_loss': 1.185830429432884}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 19/19 [00:15<00:00,  1.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 10, 'valid_acc': 0.0, 'batch_loss': 0.9319738086901213}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 63/63 [01:57<00:00,  1.87s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 11, 'train_acc': 0.0, 'train_loss': 1.1233677305872478}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 19/19 [00:14<00:00,  1.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 11, 'valid_acc': 0.0, 'batch_loss': 0.8820211040346246}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 63/63 [01:57<00:00,  1.86s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 12, 'train_acc': 0.0, 'train_loss': 1.0706949678678361}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 19/19 [00:14<00:00,  1.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 12, 'valid_acc': 0.0, 'batch_loss': 0.8633754033791391}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 63/63 [01:55<00:00,  1.83s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 13, 'train_acc': 0.0, 'train_loss': 1.0326917985128978}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 19/19 [00:15<00:00,  1.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 13, 'valid_acc': 0.0, 'batch_loss': 0.8596772677020023}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 63/63 [01:55<00:00,  1.84s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 14, 'train_acc': 0.0, 'train_loss': 0.9710533798687042}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 19/19 [00:14<00:00,  1.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 14, 'valid_acc': 0.0, 'batch_loss': 0.8021013423016197}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 63/63 [01:58<00:00,  1.89s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 15, 'train_acc': 0.0, 'train_loss': 0.9435778562984769}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 19/19 [00:14<00:00,  1.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 15, 'valid_acc': 0.0, 'batch_loss': 0.7353695960421311}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 63/63 [01:55<00:00,  1.83s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 16, 'train_acc': 0.0, 'train_loss': 0.9144247808153667}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 19/19 [00:14<00:00,  1.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 16, 'valid_acc': 0.0, 'batch_loss': 0.7154591648202193}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 63/63 [01:56<00:00,  1.85s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 17, 'train_acc': 0.0, 'train_loss': 0.8905503229489402}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 19/19 [00:15<00:00,  1.22it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 17, 'valid_acc': 0.0, 'batch_loss': 0.696841183461641}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 63/63 [01:55<00:00,  1.84s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 18, 'train_acc': 0.0, 'train_loss': 0.8778406182924906}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 19/19 [00:14<00:00,  1.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 18, 'valid_acc': 0.0, 'batch_loss': 0.6953602511631815}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 63/63 [01:59<00:00,  1.89s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 19, 'train_acc': 0.0, 'train_loss': 0.8612980800015586}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 19/19 [00:15<00:00,  1.25it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 19, 'valid_acc': 0.0, 'batch_loss': 0.693356088901821}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "def train_epoch(model,train_loader,optimizer,criterion,task,epoch,parameters):\n",
        "    train_acc = 0\n",
        "    train_loss = 0\n",
        "\n",
        "    for batch_idx, (X_batch, y_batch) in tqdm(enumerate(train_loader),total=len(train_loader)):\n",
        "        outputs = model(X_batch)\n",
        "        y_batch = y_batch.squeeze().long()\n",
        "        y_batch = F.one_hot(y_batch, num_classes=parameters[\"num_classes\"])\n",
        "\n",
        "        #print(outputs.shape)\n",
        "        #print(y_batch.shape)\n",
        "\n",
        "        loss = criterion(outputs,y_batch.float())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_acc += torch.sum(outputs == y_batch.data).float().item()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    train_acc /= len(train_loader.dataset)\n",
        "    train_loss /= len(train_loader)\n",
        "\n",
        "    return{\n",
        "        \"epoch\": epoch,\n",
        "        \"train_acc\": train_acc * 100,\n",
        "        \"train_loss\": train_loss\n",
        "    }\n",
        "\n",
        "def validate_epoch(model,valid_loader,criterion,task,epoch,parameters):\n",
        "    valid_acc = 0\n",
        "    valid_loss = 0\n",
        "\n",
        "    for batch_idx, (X_batch, y_batch) in tqdm(enumerate(valid_loader),total=len(valid_loader)):\n",
        "        with torch.no_grad():\n",
        "          outputs = model(X_batch)\n",
        "          y_batch = y_batch.squeeze().long()\n",
        "          y_batch = F.one_hot(y_batch, num_classes=parameters[\"num_classes\"])\n",
        "\n",
        "          loss = criterion(outputs,y_batch.float())\n",
        "\n",
        "          _, preds = torch.max(outputs,1)\n",
        "\n",
        "          valid_acc += torch.sum(outputs == y_batch.data).float().item()\n",
        "          valid_loss += loss.item()\n",
        "\n",
        "    valid_loss /= len(valid_loader)\n",
        "    valid_acc /= len(valid_loader.dataset)\n",
        "\n",
        "    return {\n",
        "        \"epoch\" : epoch,\n",
        "        \"valid_acc\": 100 * valid_acc,\n",
        "        \"batch_loss\": valid_loss\n",
        "    }\n",
        "\n",
        "\n",
        "def model_training(train_loader,valid_loader,parameters):\n",
        "  model = Net(parameters)\n",
        "  #model = ResNet18(parameters)\n",
        "  #model = UNetCNN(parameters)\n",
        "\n",
        "  optimizer = torch.optim.Adam(model.parameters(),lr=parameters[\"lr\"])\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  #net_model_visualizer(model)\n",
        "\n",
        "  for epoch in range(parameters[\"num_epochs\"]):\n",
        "    model.train()\n",
        "    train_history = train_epoch(model,train_loader,optimizer,criterion,task,epoch,parameters)\n",
        "    print(train_history)\n",
        "    model.eval()\n",
        "    valid_history = validate_epoch(model,valid_loader,criterion,task,epoch,parameters)\n",
        "    print(valid_history)\n",
        "\n",
        "model_training(train_loader,valid_loader,parameters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFgraWehfxGu"
      },
      "source": [
        "#Evaluation\n",
        "\n",
        "Finally, implement the evaluation of the object clasification task. You can implement any metric you want, though the most common are accuracy and AUC (one class against all for the multiclass task). You can use torch.no_grad() for speeding up predictions when no gradients are needed.\n",
        "\n",
        "How do you compare with the MedMNIST benchmarks?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NXT5ny4wVv_G"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3x3GZbJM6n9"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}