{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XRxHiKdGHiT"
      },
      "source": [
        "## Image classification with deep learning methods.\n",
        "\n",
        "-- Description --\n",
        "\n",
        "When you train the network, it is recommended to use the GPU resources of your computer.\n",
        "This will help you to learn the \"know how\" of setting up a working Python environment on your computer.\n",
        "In the case of unavailable Nvidia hardware or problems with your Python environment you can use Google Colab.\n",
        "Please go to the menu, Runtime - Change runtime type, and select **GPU** as the hardware accelerator.\n",
        "Although you used your computer successfuly it is highly recommended to give a try to Google Colab environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Eq1KWmR3HWYV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f06b54c9-f14e-4773-8877-1de4f817b0d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting medmnist\n",
            "  Downloading medmnist-3.0.1-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from medmnist) (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from medmnist) (2.0.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from medmnist) (1.2.2)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from medmnist) (0.19.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from medmnist) (4.66.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from medmnist) (9.4.0)\n",
            "Collecting fire (from medmnist)\n",
            "  Downloading fire-0.6.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.4/88.4 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from medmnist) (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from medmnist) (0.17.1+cu121)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire->medmnist) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->medmnist) (2.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->medmnist) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->medmnist) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->medmnist) (2024.1)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (1.11.4)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (3.3)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (2024.2.12)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (1.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (24.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->medmnist) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->medmnist) (3.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (1.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->medmnist)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->medmnist)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->medmnist)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->medmnist)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->medmnist)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->medmnist)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->medmnist)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->medmnist)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->medmnist)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch->medmnist)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->medmnist)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->medmnist)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->medmnist) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->medmnist) (1.3.0)\n",
            "Building wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.6.0-py2.py3-none-any.whl size=117029 sha256=9ffa2def494add3297c395340802ce3772d73d3fb69897fb660b8a57d81bea3f\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/6d/5d/5b73fa0f46d01a793713f8859201361e9e581ced8c75e5c6a3\n",
            "Successfully built fire\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fire, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, medmnist\n",
            "Successfully installed fire-0.6.0 medmnist-3.0.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "# These libraries should be sufficient for this Practice.\n",
        "# However, if any other library is needed, please install it by yourself.\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torch.utils.data as data\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import colors\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "!pip install medmnist\n",
        "import medmnist\n",
        "from medmnist import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvH6SWlDQBMf",
        "outputId": "28a56b63-f058-484b-d99d-9bfce79e69e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using downloaded and verified file: /root/.medmnist/bloodmnist.npz\n",
            "Using downloaded and verified file: /root/.medmnist/bloodmnist.npz\n",
            "Using downloaded and verified file: /root/.medmnist/bloodmnist.npz\n",
            "700\n",
            "100\n",
            "200\n"
          ]
        }
      ],
      "source": [
        "# Parameters\n",
        "NUM_EPOCHS = 20\n",
        "BATCH_SIZE = 28\n",
        "lr = 0.0001\n",
        "DOWNLOAD_OK = True\n",
        "data_flag = 'bloodmnist'\n",
        "im_size = 3*28*28\n",
        "info = INFO[data_flag]\n",
        "task = info['task']\n",
        "n_channels = info['n_channels']\n",
        "num_classes = len(info['label'])\n",
        "N_IMAGES = 1000\n",
        "data_labels = info['label']\n",
        "\n",
        "# Tupla que contiene los valores asociados a los parámetros mostrados anteriormente,\n",
        "# con el fin de mejorar la organización de dichos parámetros.\n",
        "parameters = {\"num_epochs\": NUM_EPOCHS, \"batch_size\": BATCH_SIZE, \"lr\": lr, \"download_ok\": DOWNLOAD_OK,\n",
        "              \"data_flag\": data_flag, \"im_size\": im_size,\"info_task\": task, \"n_channels\": n_channels,\n",
        "              \"num_classes\": num_classes,\"n_images\":N_IMAGES, \"data_labels\": data_labels}\n",
        "\n",
        "# Preprocesado de datos mediante la definición de la transformación de datos\n",
        "def preprocessing_data(parameters):\n",
        "  data_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[.5], std=[.5])\n",
        "  ])\n",
        "\n",
        "  full_train_dataset = BloodMNIST(split=\"train\", transform=data_transform, download=True)\n",
        "  full_valid_dataset = BloodMNIST(split=\"val\", transform=data_transform, download=True)\n",
        "  full_test_dataset = BloodMNIST(split=\"test\", transform=data_transform, download=True)\n",
        "\n",
        "  idx_train = np.random.choice(len(full_train_dataset),size=700,replace=False)\n",
        "  train_dataset = [full_train_dataset[i] for i in idx_train]\n",
        "\n",
        "  print(len(train_dataset))\n",
        "\n",
        "  idx_valid = np.random.choice(len(full_valid_dataset),size=100,replace=False)\n",
        "  valid_dataset = [full_valid_dataset[i] for i in idx_valid]\n",
        "\n",
        "  print(len(valid_dataset))\n",
        "\n",
        "  idx_test = np.random.choice(len(full_test_dataset),size=200,replace=False)\n",
        "  test_dataset = [full_test_dataset[i] for i in idx_test]\n",
        "\n",
        "\n",
        "  train_loader = data.DataLoader(dataset=train_dataset, batch_size=parameters[\"batch_size\"], shuffle=True)\n",
        "  valid_loader = data.DataLoader(dataset=valid_dataset, batch_size=parameters[\"batch_size\"], shuffle=False)\n",
        "  test_loader = data.DataLoader(dataset=test_dataset, batch_size=parameters[\"batch_size\"], shuffle=True)\n",
        "\n",
        "  return train_loader, valid_loader, test_loader\n",
        "\n",
        "train_loader,valid_loader,test_loader = preprocessing_data(parameters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMVmgmpHGrtx"
      },
      "outputs": [],
      "source": [
        "# Your code\n",
        "\n",
        "# Función que permite visualizar los aspectos fundamentales sobre cada\n",
        "# dataset que se haya descargado/cargado previamente.\n",
        "def dataset_visualizer(dataset,length_montage):\n",
        "\n",
        "  for i in range(0,length_montage*length_montage):\n",
        "    img = dataset[i][0]\n",
        "    label = str(dataset[i][1]).replace('[','')\n",
        "    figure = plt.figure(figsize=(2,2))\n",
        "    plt.imshow(img.permute(1,2,0))\n",
        "    plt.title(data_labels[label.replace(']','')])\n",
        "    plt.axis(\"off\")\n",
        "  plt.show()\n",
        "\n",
        "# Visualizador de las imágenes a través de un pipeline DataLoader\n",
        "def dataloader_visualizer(dataset,num_batches):\n",
        "  data_loader = data.DataLoader(dataset,batch_size=28,shuffle=True)\n",
        "  for batch_idx, (features, labels) in enumerate(data_loader):\n",
        "      if batch_idx >= num_batches:\n",
        "        break\n",
        "      for i in range(len(features)):\n",
        "        img = features[i].squeeze()\n",
        "        label = str(labels[i]).replace('tensor([','').replace('])','')\n",
        "        plt.figure(figsize=(2,2))\n",
        "        plt.title(label)\n",
        "        plt.imshow(img.permute(1,2,0))\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "dataloader_visualizer(blood_train_dataset,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2z44gQHEUhry",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "5e4538a8-0142-42f7-a810-9cd0dca3bad1"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'INFO' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-79f613708875>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mN_IMAGES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mINFO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_flag\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'task'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mn_channels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'n_channels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'INFO' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Función que permite encapsular la carga de los datasets\n",
        "def dataset_loading(train_dataset,valid_dataset,test_dataset,parameters):\n",
        "  train_loader = data.DataLoader(dataset=train_dataset, batch_size=parameters[\"batch_size\"], shuffle=True)\n",
        "  valid_loader = data.DataLoader(dataset=valid_dataset, batch_size=parameters[\"batch_size\"], shuffle=False)\n",
        "  test_loader = data.DataLoader(dataset=test_dataset, batch_size=parameters[\"batch_size\"], shuffle=True)\n",
        "\n",
        "  return train_loader, valid_loader, test_loader\n",
        "\n",
        "# Función que representa la métrica de error asociada al modelo predictivo\n",
        "def loss_function(task):\n",
        "  return torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Función que representa el optimizador utilizado para el modelo predictivo\n",
        "def model_optimizer(model):\n",
        "  #return optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "  return torch.optim.Adam(model.parameters(),lr=lr)\n",
        "\n",
        "train_loader,valid_loader,test_loader = dataset_loading(blood_train_dataset,\n",
        "                                                        blood_valid_dataset,\n",
        "                                                        blood_test_dataset,\n",
        "                                                        parameters)\n",
        "\n",
        "print(len(train_loader))\n",
        "print(len(valid_loader))\n",
        "print(len(test_loader))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dn-gr3Y1dhc0"
      },
      "source": [
        "#Create a deep learning model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "X8OH_2iLU6oZ"
      },
      "outputs": [],
      "source": [
        "# Define a simple CNN model\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, in_channels, num_classes, im_size):\n",
        "        super(Net, self).__init__()\n",
        "        #Define the desired deep learning model\n",
        "        #Your code\n",
        "\n",
        "        #Primera capa del modelo de red convolucional\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(n_channels,10,kernel_size=3,padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        #Segunda capa del modelo de red convolucional\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(10,50,kernel_size=3,padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2,2),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "\n",
        "        #Tercera capa del modelo de red convolucional\n",
        "        #self.layer3 = nn.Sequential(\n",
        "            #nn.Conv2d(20, 30, kernel_size=3),\n",
        "            #nn.BatchNorm2d(64),\n",
        "            #nn.ReLU()\n",
        "        #)\n",
        "\n",
        "        #Cuarta capa del modelo de red convolucional\n",
        "        #self.layer4 = nn.Sequential(\n",
        "            #nn.Conv2d(30, 40, kernel_size=3),\n",
        "            #nn.BatchNorm2d(64),\n",
        "            #nn.ReLU()\n",
        "        #)\n",
        "\n",
        "        #Quinta capa del modelo de red convolucional\n",
        "        #self.layer5 = nn.Sequential(\n",
        "            #nn.Conv2d(40, 50, kernel_size=3, padding=1),\n",
        "            #nn.BatchNorm2d(64),\n",
        "            #nn.ReLU(),\n",
        "            #nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        #)\n",
        "\n",
        "        # Capa FC (full-conected)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(14*14*50,28)\n",
        "        )\n",
        "\n",
        "        #End your code\n",
        "\n",
        "    def forward(self, x):\n",
        "        #Your code\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        #End your code\n",
        "\n",
        "def net_model_visualizer(net_model):\n",
        "  print(net_model)\n",
        "  print('Total Parameters:',\n",
        "       sum([torch.numel(p) for p in net_model.parameters()])\n",
        "  )\n",
        "  print('Trainable Parameters:',\n",
        "       sum([torch.numel(p) for p in net_model.parameters() if p.requires_grad])\n",
        "  )\n",
        "\n",
        "#model = Net(in_channels=n_channels, num_classes=num_classes, im_size = im_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENrFXnaOVt0i"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "def train_epoch(model, train_loader, optimizer, criterion, task):\n",
        "\n",
        "    correct = 0\n",
        "    total_loss = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for batch_idx, (X, y) in enumerate(train_loader):\n",
        "        model.train()\n",
        "        pred = model(X)\n",
        "\n",
        "        loss = criterion(pred,y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        _, predicted = torch.max(pred, 1)\n",
        "        total_samples += y.size(0)\n",
        "        correct += (predicted == y).sum().item()\n",
        "        total_loss += loss.item() * y.size(0)\n",
        "\n",
        "    return {\n",
        "        \"train_acc\": correct / total_samples,\n",
        "        \"train_loss\": total_loss / total_samples,\n",
        "    }\n",
        "\n",
        "def centralized_training(train_loader,test_loader,parameters):\n",
        "  centralized_model = Net(in_channels=parameters[\"n_channels\"],\n",
        "                            num_classes=parameters[\"num_classes\"],\n",
        "                            im_size=parameters[\"im_size\"])\n",
        "\n",
        "  net_model_visualizer(centralized_model)\n",
        "  optimizer = torch.optim.Adam(centralized_model.parameters(),lr=lr)\n",
        "  criterion = torch.nn.MSELoss()\n",
        "\n",
        "  for epoch in range(parameters[\"num_epochs\"]):\n",
        "    train_history = train_epoch(centralized_model,train_loader,optimizer,criterion,task)\n",
        "    #valid_history = validate_epoch(centralized_model,test_loader,criterion,task)\n",
        "    #print(f'Epoch {epoch}: {train_history} - {valid_history}')\n",
        "\n",
        "\n",
        "\n",
        "centralized_training(train_loader,test_loader,parameters)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F6sIqmxhkTZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFgraWehfxGu"
      },
      "source": [
        "#Evaluation\n",
        "\n",
        "Finally, implement the evaluation of the object clasification task. You can implement any metric you want, though the most common are accuracy and AUC (one class against all for the multiclass task). You can use torch.no_grad() for speeding up predictions when no gradients are needed.\n",
        "\n",
        "How do you compare with the MedMNIST benchmarks?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "NXT5ny4wVv_G",
        "outputId": "f190a9b2-67c1-4a45-de6b-5f374f508ddd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Evaluating ...\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'test' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-0da269987213>\u001b[0m in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'==> Evaluating ...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'test' is not defined"
          ]
        }
      ],
      "source": [
        "# Evaluation\n",
        "\n",
        "# Your code\n",
        "def model_evaluation_dataset(split):\n",
        "  model.eval()\n",
        "\n",
        "  y_true = torch.tensor([])\n",
        "  y_score = torch.tensor([])\n",
        "\n",
        "  data_loader = train_loader_at_eval if split == 'train' else test_loader\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for inputs, targets in data_loader:\n",
        "      outputs = model(inputs)\n",
        "\n",
        "      if task == 'multi-label, binary-class':\n",
        "        targets = targets.to(torch.float32)\n",
        "        outputs = outputs.softmax(dim=-1)\n",
        "      else:\n",
        "        targets = targets.squeeze().long()\n",
        "        outputs = outputs.softmax(dim=-1)\n",
        "        targets = targets.float().resize_(len(targets), 1)\n",
        "\n",
        "      y_true = torch.cat((y_true, targets), 0)\n",
        "      y_score = torch.cat((y_score, outputs), 0)\n",
        "\n",
        "    y_true = y_true.numpy()\n",
        "    y_score = y_score.detach().numpy()\n",
        "\n",
        "    evaluator = Evaluator(data_flag, split)\n",
        "    metrics = evaluator.evaluate(y_score)\n",
        "\n",
        "    print('%s  auc: %.3f  acc:%.3f' % (split, *metrics))\n",
        "\n",
        "print('==> Evaluating ...')\n",
        "model_evaluation_dataset('test')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3x3GZbJM6n9"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}